ä»¥ä¸‹æ˜¯æ•´åˆäº†è¯¦ç»†æ³¨é‡Šçš„å®Œæ•´ä»£ç ï¼Œä»¥åŠå¯¹åº”çš„è¯„ä¼°æŒ‡æ ‡å’Œå›¾è¡¨è¯´æ˜ï¼š

```python
# ===================== ç¯å¢ƒé…ç½® =====================
# è§£å†³ä¸­æ–‡æ˜¾ç¤ºå’Œé”™è¯¯çš„æ ¸å¿ƒè®¾ç½®
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’æ¨¡å¼é¿å…GUIæŠ¥é”™
import matplotlib.pyplot as plt

# ä¸­æ–‡å­—ä½“é…ç½®ï¼ˆWindowsç³»ç»Ÿæ¨èä½¿ç”¨SimHeiï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºå¼‚å¸¸

# ===================== ä¾èµ–åº“å¯¼å…¥ =====================
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from pytorch_tabnet.tab_model import TabNetClassifier

# ===================== æ•°æ®é¢„å¤„ç† =====================
def preprocess_data():
    """
    æ•°æ®é¢„å¤„ç†æµç¨‹ï¼š
    1. åŠ è½½åŸå§‹æ•°æ®å¹¶è®¾ç½®åˆ—å
    2. å¤„ç†æ•°å€¼å‹å¼‚å¸¸å€¼ï¼ˆ0å€¼æ›¿æ¢ä¸ºä¸­ä½æ•°ï¼‰
    3. ç±»å‹è½¬æ¢å’Œç¼ºå¤±å€¼å¤„ç†
    """
    # åŠ è½½æ— åˆ—åçš„åŸå§‹æ•°æ®
    df = pd.read_csv('pima-indians-diabetes.data', header=None)
    
    # è®¾ç½®æ­£ç¡®çš„åˆ—åï¼ˆæ ¹æ®æ•°æ®é›†æ–‡æ¡£ï¼‰
    columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
               'Insulin', 'BMI', 'DiabetesPedigree', 'Age', 'Outcome']
    df.columns = columns
    
    # å¼ºåˆ¶è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼ˆå¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸²å‹æ•°å€¼ï¼‰
    numeric_cols = columns[:-1]
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # è½¬æ¢å¤±è´¥è®¾ä¸ºNaN
    
    # å¤„ç†0å€¼å¼‚å¸¸ï¼ˆåŒ»å­¦æŒ‡æ ‡ä¸å¯èƒ½ä¸º0çš„å­—æ®µï¼‰
    zero_fields = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    for field in zero_fields:
        mask = df[field] == 0
        df.loc[mask, field] = np.nan  # å°†0å€¼æ›¿æ¢ä¸ºNaN
        median = df[field].median()  # è®¡ç®—ä¸­ä½æ•°
        df[field] = df[field].fillna(median)  # ä¸­ä½æ•°å¡«å……
    
    # æ¸…é™¤å‰©ä½™ç¼ºå¤±å€¼å¹¶è½¬æ¢æ ‡ç­¾ç±»å‹
    df = df.dropna()
    df['Outcome'] = df['Outcome'].astype(int)  # ç¡®ä¿æ ‡ç­¾ä¸ºæ•´æ•°ç±»å‹
    return df

# ===================== å¯è§†åŒ–å‡½æ•° =====================
def plot_confusion_matrix(y_true, y_pred, model_name):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜ä¸ºå›¾ç‰‡"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"{model_name}æ··æ·†çŸ©é˜µ")
    plt.xlabel("é¢„æµ‹æ ‡ç­¾")
    plt.ylabel("çœŸå®æ ‡ç­¾")
    plt.savefig(f"{model_name}_confusion_matrix.png")
    plt.close()

def plot_feature_importance(importances, features, model_name):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆé€‚ç”¨äºæ ‘æ¨¡å‹ï¼‰"""
    indices = np.argsort(importances)
    plt.figure(figsize=(10, 6))
    plt.title(f"{model_name}ç‰¹å¾é‡è¦æ€§")
    plt.barh(range(len(indices)), importances[indices], align="center")
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    plt.xlabel("é‡è¦æ€§å¾—åˆ†")
    plt.savefig(f"{model_name}_feature_importance.png")
    plt.close()

def plot_coefficients(coef, features, model_name):
    """ç»˜åˆ¶ç³»æ•°ç»å¯¹å€¼å›¾ï¼ˆé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼‰"""
    indices = np.argsort(np.abs(coef))
    plt.figure(figsize=(10, 6))
    plt.title(f"{model_name}ç‰¹å¾ç³»æ•°")
    plt.barh(range(len(indices)), np.abs(coef[indices]), align="center")
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    plt.xlabel("ç³»æ•°ç»å¯¹å€¼")
    plt.savefig(f"{model_name}_coefficients.png")
    plt.close()

# ===================== ä¸»ç¨‹åº =====================
def main():
    # æ•°æ®é¢„å¤„ç†
    df = preprocess_data()
    print("æ•°æ®æ ·ä¾‹ï¼š\n", df.head())
    print("\næ•°æ®ç±»å‹ï¼š\n", df.dtypes)

    # ç‰¹å¾ä¸æ ‡ç­¾åˆ†ç¦»
    X = df.drop('Outcome', axis=1)
    y = df['Outcome']

    # æ•°æ®åˆ†å‰²ï¼ˆåˆ†å±‚æŠ½æ ·ä¿è¯ç±»åˆ«æ¯”ä¾‹ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42, 
        stratify=y  # ä¿æŒç±»åˆ«åˆ†å¸ƒ
    )

    # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæå‡æ¨¡å‹æ”¶æ•›é€Ÿåº¦ï¼‰
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    features = X.columns.tolist()  # ä¿å­˜ç‰¹å¾åç§°ç”¨äºå¯è§†åŒ–

    # æ¨¡å‹å®šä¹‰ï¼ˆåŒ…å«ä¸åŒç®—æ³•ï¼‰
    models = {
        "é€»è¾‘å›å½’": LogisticRegression(max_iter=1000),  # å¢åŠ è¿­ä»£æ¬¡æ•°ç¡®ä¿æ”¶æ•›
        "å†³ç­–æ ‘": DecisionTreeClassifier(max_depth=5),  # é™åˆ¶æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
        "éšæœºæ£®æ—": RandomForestClassifier(n_estimators=100),  # 100æ£µæ ‘çš„æ£®æ—
        "XGBoost": XGBClassifier(eval_metric="logloss", enable_categorical=False),  # ç¦ç”¨ç±»åˆ«å‹ç‰¹å¾
        "TabNet": TabNetClassifier(
            n_d=16,   # å†³ç­–å±‚ç»´åº¦
            n_a=16,   # æ³¨æ„åŠ›å±‚ç»´åº¦
            gamma=1.3 # æ­£åˆ™åŒ–ç³»æ•°
        )
    }

    # æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
    for name, model in models.items():
        print(f"\n{'=' * 30}\næ­£åœ¨è®­ç»ƒ {name} æ¨¡å‹...")

        try:
            # TabNetç‰¹æ®Šå¤„ç†
            if name == "TabNet":
                model.fit(
                    X_train.astype(np.float32),  # è½¬æ¢ä¸º32ä½æµ®ç‚¹æ•°
                    y_train.values.astype(int).ravel(),  # ç¡®ä¿æ ‡ç­¾ä¸ºæ•´æ•°
                    eval_set=[(X_test.astype(np.float32), y_test.values.astype(int).ravel())],
                    max_epochs=100,    # æœ€å¤§è®­ç»ƒè½®æ¬¡
                    patience=20,       # æ—©åœç­‰å¾…è½®æ•°
                    batch_size=64,     # æ‰¹æ¬¡å¤§å°
                    virtual_batch_size=32,  # è™šæ‹Ÿæ‰¹æ¬¡å¤§å°
                    eval_metric=['auc']  # ä½¿ç”¨AUCä½œä¸ºè¯„ä¼°æŒ‡æ ‡
                )
                y_pred = model.predict(X_test.astype(np.float32)).flatten()
            else:
                # å¸¸è§„æ¨¡å‹è®­ç»ƒ
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)

            # æ¨¡å‹è¯„ä¼°
            print(f"\n{name} è¯„ä¼°ç»“æœ:")
            print("å‡†ç¡®ç‡:", accuracy_score(y_test, y_pred))
            print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_test, y_pred))

            # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
            plot_confusion_matrix(y_test, y_pred, name)
            
            # ç‰¹å¾é‡è¦æ€§/ç³»æ•°å¯è§†åŒ–
            if hasattr(model, "feature_importances_"):
                plot_feature_importance(model.feature_importances_, features, name)
            elif hasattr(model, "coef_"):
                plot_coefficients(model.coef_[0], features, name)

        except Exception as e:
            print(f"{name} è®­ç»ƒå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()
    print("\næ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³å½“å‰ç›®å½•ã€‚")
```

---

### ğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯´æ˜

1. **å‡†ç¡®ç‡ (Accuracy)**
   - **å…¬å¼**ï¼š`(æ­£ç¡®é¢„æµ‹æ•°) / (æ€»æ ·æœ¬æ•°)`
   - **è§£è¯»**ï¼šæ•´ä½“é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ï¼Œä½†å¯èƒ½å—ç±»åˆ«ä¸å¹³è¡¡å½±å“

2. **åˆ†ç±»æŠ¥å‘Š (Classification Report)**
   - **ç²¾ç¡®ç‡ (Precision)**
     - **å…¬å¼**ï¼š`TP / (TP + FP)`
     - **æ„ä¹‰**ï¼šé¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­å®é™…ä¸ºæ­£çš„æ¯”ä¾‹ï¼ˆæŸ¥å‡†ç‡ï¼‰
   - **å¬å›ç‡ (Recall)**
     - **å…¬å¼**ï¼š`TP / (TP + FN)`
     - **æ„ä¹‰**ï¼šå®é™…ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆæŸ¥å…¨ç‡ï¼‰
   - **F1-Score**
     - **å…¬å¼**ï¼š`2 * (Precision * Recall) / (Precision + Recall)`
     - **æ„ä¹‰**ï¼šç»¼åˆè€ƒé‡ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„å¹³è¡¡æŒ‡æ ‡
   - **Support**ï¼šå„ç±»åˆ«çš„å®é™…æ ·æœ¬æ•°é‡

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
              precision    recall  f1-score   support

           0       0.80      0.85      0.82       100
           1       0.69      0.62      0.65        53

    accuracy                           0.76       153
   macro avg       0.74      0.73      0.74       153
weighted avg       0.76      0.76      0.76       153
```

---

### ğŸ“ˆ å›¾è¡¨è¯´æ˜

1. **æ··æ·†çŸ©é˜µ (Confusion Matrix)**
   - **æ¨ªè½´**ï¼šæ¨¡å‹é¢„æµ‹çš„æ ‡ç­¾
   - **çºµè½´**ï¼šçœŸå®æ ‡ç­¾
   - **è§£è¯»**ï¼š
     - å·¦ä¸Šï¼ˆTNï¼‰ï¼šæ­£ç¡®é¢„æµ‹çš„è´Ÿç±»æ ·æœ¬æ•°
     - å³ä¸‹ï¼ˆTPï¼‰ï¼šæ­£ç¡®é¢„æµ‹çš„æ­£ç±»æ ·æœ¬æ•°
     - å…¶ä»–å•å…ƒæ ¼æ˜¾ç¤ºé”™è¯¯é¢„æµ‹æƒ…å†µ
2. **ç‰¹å¾é‡è¦æ€§ (Feature Importance)**
   - **é€‚ç”¨æ¨¡å‹**ï¼šå†³ç­–æ ‘ã€éšæœºæ£®æ—ã€XGBoost
   - **è§£è¯»**ï¼šæ¡å½¢è¶Šé•¿è¡¨ç¤ºè¯¥ç‰¹å¾å¯¹é¢„æµ‹ç»“æœå½±å“è¶Šå¤§
   - **åº”ç”¨**ï¼šè¯†åˆ«å…³é”®å½±å“å› ç´ ï¼Œè¾…åŠ©ç‰¹å¾é€‰æ‹©
3. **ç³»æ•°å›¾ (Coefficients)**
   - **é€‚ç”¨æ¨¡å‹**ï¼šé€»è¾‘å›å½’
   - **è§£è¯»**ï¼š
     - æ­£å€¼è¡¨ç¤ºç‰¹å¾ä¸æ­£ç±»ï¼ˆæ‚£ç—…ï¼‰æ­£ç›¸å…³
     - è´Ÿå€¼è¡¨ç¤ºç‰¹å¾ä¸è´Ÿç±»ï¼ˆå¥åº·ï¼‰æ­£ç›¸å…³
   - **æ³¨æ„**ï¼šéœ€é…åˆæ ‡å‡†åŒ–æ•°æ®è§£è¯»

---

è¯¥ä»£ç å®ç°äº†ä»æ•°æ®æ¸…æ´—åˆ°æ¨¡å‹è§£é‡Šçš„å®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹ï¼Œé€‚ç”¨äºäºŒåˆ†ç±»åŒ»ç–—é¢„æµ‹ä»»åŠ¡ã€‚è¾“å‡ºç»“æœåŒ…æ‹¬ï¼š
- é¢„å¤„ç†åçš„æ•°æ®æ ·ä¾‹
- å„æ¨¡å‹çš„å‡†ç¡®ç‡å’Œè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
- æ··æ·†çŸ©é˜µå’Œç‰¹å¾ç›¸å…³æ€§çš„å¯è§†åŒ–å›¾è¡¨
